<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yifei Wang </title> <meta name="author" content="Yifei Wang"> <meta name="description" content="Postdoc at MIT CSAIL. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon-192x192.png?df45a378aa715da8c29ebd47ed713002"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yifeiwang77.github.io/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">service </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks </a> </li> <li class="nav-item "> <a class="nav-link" href="/misc/">misc </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yifei</span> Wang <span style="color: var(--global-theme-color);">(on the job market!) </span> </h1> <p class="desc">Postdoctoral researcher at <a href="https://www.csail.mit.edu/" rel="external nofollow noopener" target="_blank">MIT CSAIL</a>, advised by <a href="https://people.csail.mit.edu/stefje/" rel="external nofollow noopener" target="_blank">Stefanie Jegelka</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/avatar.JPG" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/avatar.JPG?27fe5d7b4db363c35628ecea99112d66" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="avatar.JPG" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>My goal is to develop models that learn from massive data with minimal human efforts, which drives my persistent interests in self-supervised foundation models. My research has contributed to unveiling the key principles underlying these foundation models and designing efficient algorithms to improve their capabilities and safety: </p> <ul> <li> <strong>Mathematical Principles of Foundation Models</strong>. We established theoretical foundations for a broad spectrum of <em>Self-Supervised Learning (SSL)</em> methods that are at the heart of foundation models, from contrastive [<a href="http://arxiv.org/pdf/2203.13457" rel="external nofollow noopener" target="_blank">1</a>, <a href="https://openreview.net/pdf?id=VBTJqqWjxMv" rel="external nofollow noopener" target="_blank">2</a>], non-contrastive [<a href="https://openreview.net/pdf?id=cIbjyd2Vcy" rel="external nofollow noopener" target="_blank">3</a>], autoregressive [<a href="https://openreview.net/pdf?id=2rPoTgEmjV" rel="external nofollow noopener" target="_blank">4</a>], reconstructive [<a href="https://arxiv.org/pdf/2210.08344" rel="external nofollow noopener" target="_blank">5</a>], to predictive [<a href="https://openreview.net/pdf?id=yLpuruMZHE" rel="external nofollow noopener" target="_blank">6</a>] approaches. Our recent work further pioneered the first rigorous theory [<a href="https://arxiv.org/pdf/2405.18634" rel="external nofollow noopener" target="_blank">7</a>] for the <em>test-time self-correction ability</em> of LLMs, a key mechanism for LLM reasoning.</li> <li> <strong>Improving Model Capabilities</strong>. We leveraged these principles to “debug” and “boost” foundation models. We generalized self-supervised learning to be able to self-adapt to new tasks without retraining [<a href="https://arxiv.org/pdf/2405.18193" rel="external nofollow noopener" target="_blank">8</a>] (<a href="https://www.csail.mit.edu/news/machines-self-adapt-new-tasks-without-re-training" rel="external nofollow noopener" target="_blank">featured by MIT</a>), proposed adaptive use of AI-generated data to circumvent data shortage [<a href="https://arxiv.org/pdf/2403.12448.pdf" rel="external nofollow noopener" target="_blank">9</a>], and significantly enhanced LLMs’ long-context understanding through self-identifying key tokens [<a href="https://arxiv.org/pdf/2410.23771" rel="external nofollow noopener" target="_blank">10</a>].</li> <li> <strong>Safe and Trustworthy AI</strong>. We developed principled understandings and algorithms for adversarial robustness [<a href="http://arxiv.org/pdf/2203.13455" rel="external nofollow noopener" target="_blank">11</a>, <a href="https://arxiv.org/pdf/2210.07540.pdf" rel="external nofollow noopener" target="_blank">12</a>, <a href="https://arxiv.org/pdf/2310.19360.pdf" rel="external nofollow noopener" target="_blank">13</a>, <a href="https://arxiv.org/pdf/2310.18936.pdf" rel="external nofollow noopener" target="_blank">14</a>], interpretability [<a href="https://arxiv.org/pdf/2310.18904.pdf" rel="external nofollow noopener" target="_blank">15</a>, <a href="https://arxiv.org/pdf/2403.12459" rel="external nofollow noopener" target="_blank">16</a>], and domain generalization [<a href="https://arxiv.org/pdf/2210.06807" rel="external nofollow noopener" target="_blank">17</a>, <a href="https://arxiv.org/pdf/2212.09082.pdf" rel="external nofollow noopener" target="_blank">18</a>, <a href="https://arxiv.org/pdf/2310.12793" rel="external nofollow noopener" target="_blank">19</a>]. In DynACL [<a href="https://arxiv.org/abs/2303.01289" rel="external nofollow noopener" target="_blank">20</a>], we built the first self-supervised model that is as robust as the supervised one. We firstly showed that LLMs’ core emergent abilities, in-context learning [<a href="https://arxiv.org/pdf/2310.06387" rel="external nofollow noopener" target="_blank">21</a>] and self-correction [<a href="https://arxiv.org/pdf/2405.18634" rel="external nofollow noopener" target="_blank">7</a>], can play important roles in safety tasks like jailbreaking, which was <a href="https://www.anthropic.com/research/many-shot-jailbreaking" rel="external nofollow noopener" target="_blank">featured and scaled up by Anthropic</a>.</li> </ul> <p>My first-author papers received the Best ML Paper Award (1/685) at ECML-PKDD 2021, the Silver Best Paper Award at ICML 2021 workshop, and the Best Paper Award at ICML 2024 workshop. My thesis won CAAI Outstanding Ph.D. Dissertation Runner-Up Award. I have published 43 peer-reviewed papers (38 in NeurIPS, ICLR, and ICML), and I am a (co-)first author on 28 of them.</p> <p>I served as an organizer for <a href="https://redteaming-gen-ai.github.io/" rel="external nofollow noopener" target="_blank">NeurIPS 2024 Workshop on Red Teaming GenAI</a> and the <a href="https://projects.csail.mit.edu/ml-tea/" rel="external nofollow noopener" target="_blank">ML Tea Seminar at MIT</a>. I served as an Area Chair for ICLR 2024 and 2025, and as a reviewer for main AI conferences (NeurIPS, ICML, ECML, AISTATS, LoG, CVPR, ACL).</p> <p>I obtained my PhD in Applied Mathematics from Peking University in 2023, advised by <a href="https://yisenwang.github.io" rel="external nofollow noopener" target="_blank">Yisen Wang</a>, <a href="https://zhouchenlin.github.io/" rel="external nofollow noopener" target="_blank">Zhouchen Lin</a>, <a href="https://www.math.pku.edu.cn/jsdw/js_20180628175159671361/y_20180628175159671361/69984.htm" rel="external nofollow noopener" target="_blank">Jiansheng Yang</a>. Prior to that, I did my undergraduate at School of Mathematical Sciences, Peking University.</p> <p><strong>I am on the job market 2024-2025 and actively looking for jobs! Links: <a href="assets/pdf/CV-Yifei-Wang-MIT.pdf">CV</a> | <a href="assets/pdf/Research%20Statement%20Yifei%20Wang.pdf">Research Statement</a></strong></p> <p>You may find some of my recent research highlights on <a href="https://x.com/yifeiwang77/highlights" rel="external nofollow noopener" target="_blank">X/Twitter</a>.</p> <div class="social"> <div class="contact-icons"> <a href="mailto:%79%69%66%65%69_%77@%6D%69%74.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=-CLy6YsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/yifeiwang77" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yifeiwang77" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/yifeiwang77" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"></div> </div> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">April, 2025</th> <td> I will be giving a talk at the <a href="https://www.cs.stonybrook.edu/cse-600-seminar-self-learning-principles-large-scale-foundation-models" rel="external nofollow noopener" target="_blank">CSE 600 Seminar</a> at Stony Brook University on April 18. </td> </tr> <tr> <th scope="row" style="width: 20%">April, 2025</th> <td> Our recent work <a href="https://arxiv.org/pdf/2502.07266" rel="external nofollow noopener" target="_blank">When More is Less: Understanding Chain-of-Thought Length in LLMs</a> has been selected as an <strong>Oral presentation</strong> at <a href="https://workshop-llm-reasoning-planning.github.io/" rel="external nofollow noopener" target="_blank">ICLR 2025 Workshop on Reasoning and Planning for LLMs</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">March, 2025</th> <td> Our recent work <a href="https://arxiv.org/abs/2503.01776" rel="external nofollow noopener" target="_blank">Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</a> has sparked <a href="https://x.com/yifeiwang77/status/1897023662328611062" rel="external nofollow noopener" target="_blank">significant interest</a> as a promising alternative paradigm for efficient embedding retrieval. </td> </tr> <tr> <th scope="row" style="width: 20%">January, 2025</th> <td> I will give a talk at the University of Michigan on Feb 12. </td> </tr> <tr> <th scope="row" style="width: 20%">January, 2025</th> <td> 5 papers were accepted at ICLR 2025 (3 as a co-first author)! We proposed <a href="https://openreview.net/pdf?id=fL4qWkSmtM" rel="external nofollow noopener" target="_blank">long-context perplexity</a> and <a href="https://openreview.net/pdf?id=q1UyoY3MgJ" rel="external nofollow noopener" target="_blank">invariant in-context learning</a> for better training and usage of LLMs. We also looked into some fundamental questions, such as <a href="https://openreview.net/pdf?id=INe4otjryz" rel="external nofollow noopener" target="_blank">OOD generalization of in-context learning</a>, <a href="https://openreview.net/forum?id=g6Qc3p7JH5" rel="external nofollow noopener" target="_blank">interplay between monosemanticity and robustness</a>, and <a href="https://openreview.net/pdf?id=L0evcuybH5" rel="external nofollow noopener" target="_blank">the nature of projection heads</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">January, 2025</th> <td> I will give a talk at NYU CDS on Feb 5. </td> </tr> <tr> <th scope="row" style="width: 20%">December, 2024</th> <td> Our NeurIPS’24 work <a href="https://arxiv.org/pdf/2405.18193" rel="external nofollow noopener" target="_blank">ContextSSL</a> was featured by MIT 📰: <em><a href="https://www.csail.mit.edu/news/machines-self-adapt-new-tasks-without-re-training" rel="external nofollow noopener" target="_blank">Machines that Self-adapt to New Tasks without Re-training</a></em>. It was also selected as an oral presentation (top 4) at <a href="https://sslneurips2024.github.io/" rel="external nofollow noopener" target="_blank">NeurIPS’24 SSL workshop</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">October, 2024</th> <td> 6 papers were accepted to NeurIPS 2024. We inverstigated <em>how LLMs perform self-correction at test time</em> (<a href="https://arxiv.org/pdf/2405.18634" rel="external nofollow noopener" target="_blank">paper</a>), how to build <em>dynamic world models</em> through joint embedding methods (<a href="https://arxiv.org/pdf/2405.18193" rel="external nofollow noopener" target="_blank">paper</a>), how Transformers avoid feature collapse with LayerNorm and attention masks (<a href="https://arxiv.org/pdf/2405.18781" rel="external nofollow noopener" target="_blank">paper</a>), and why equivariant prediction of data corruptions helps learn good representations (<a href="https://openreview.net/pdf?id=NLqdudgBfy" rel="external nofollow noopener" target="_blank">paper</a>). </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> <special2 class="badge rounded w-100">LLM training and eval</special2> </div> <div id="fang2024perplexity" class="col-sm-8"> <div class="title">What is Wrong with Perplexity for Long-context Language Modeling?</div> <div class="author"> Lizhe Fang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang </div> <div class="periodical"> <em>ICLR</em>, 2025 </div> <div class="periodical"> We proposed a <strong>long-context perplexity (LongPPL)</strong> measure that emphasizes long-context relevant tokens at training and evaluation, improving benchmark scores on LongBench, LongEval, and RULER by <strong>up to 22%</strong>. </div> <div class="links"> <a href="https://openreview.net/pdf?id=fL4qWkSmtM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/LongPPL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <special class="badge rounded w-100">Best Paper Award<br> at ICML-W’24</special> </div> <div id="wang2024theoretical" class="col-sm-8"> <div class="title">A Theoretical Understanding of Self-Correction through In-context Alignment</div> <div class="author"> <u>Yifei Wang</u><sup>*</sup>, Yuyang Wu<sup>*</sup>, Zeming Wei, Stefanie Jegelka, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> <span style="font-weight: bold;">🏆 Best Paper Award</span> at ICML 2024 ICL Workshop<br> We proposed the first theoretical explanation of how LLM self-correction works (as in OpenAI o1) and showed its effectiveness against social bias and jailbreak attacks. </div> <div class="links"> <a href="https://arxiv.org/pdf/2405.18634" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/yifeiwang77/Self-Correction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <special class="badge rounded w-100">Oral at NeurIPS-W’24</special> <special2 class="badge rounded w-100">Featured by MIT</special2> </div> <div id="wang2024symmetries" class="col-sm-8"> <div class="title">In-Context Symmetries: Self-Supervised Learning through Contextual World Models</div> <div class="author"> Sharut Gupta<sup>*</sup>, Chenyu Wang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Tommi Jaakkola, and Stefanie Jegelka </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> <span style="font-weight: bold;">Oral Presentation (top 4)</span> at NeurIPS 2024 SSL Workshop &amp; <span style="font-weight: bold;">featured by <a href="https://www.csail.mit.edu/news/machines-self-adapt-new-tasks-without-re-training" rel="external nofollow noopener" target="_blank">MIT 📰</a></span><br> We introduced unsupervised test-time adaptation ability to self-supervised learning through a contextual world model designed for joint embedding (JEPA) models. </div> <div class="links"> <a href="https://arxiv.org/pdf/2405.18193" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.csail.mit.edu/news/machines-self-adapt-new-tasks-without-re-training" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/Sharut/In-Context-Symmetries" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <special2 class="badge rounded w-100">Featured by Anthropic</special2> </div> <div id="wei2023jailbreak" class="col-sm-8"> <div class="title">Jailbreak and guard aligned language models with only few in-context demonstrations</div> <div class="author"> Zeming Wei,  <u>Yifei Wang</u>, and Yisen Wang </div> <div class="periodical"> <em>arXiv preprint arXiv:2310.06387</em>, 2023 </div> <div class="periodical"> <span style="font-weight: bold;">Cited over 160 times</span>. Featured and scaled up in <strong><a href="https://www.anthropic.com/research/many-shot-jailbreaking" target="_blank" rel="external nofollow noopener">Anthropic’s blog 📰</a></strong>, where in-context attack successfully jailbroke prominent LLMs including GPT and Claude. </div> <div class="links"> <a href="https://arxiv.org/pdf/2310.06387" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="wang2024nonnegative" class="col-sm-8"> <div class="title">Non-negative Contrastive Learning</div> <div class="author"> <u>Yifei Wang</u><sup>*</sup>, Qi Zhang<sup>*</sup>, Yaoyu Guo, and Yisen Wang </div> <div class="periodical"> <em>In ICLR</em>, 2024 </div> <div class="periodical"> Inspired by NMF, we introduced a simple technique (one-line) that <span style="font-weight: bold;">attains 90% feature sparsity and 10x feature interpretability</span> for self-supervised contrastive learning, with theoretical guarantees on its disentanglement and performance. </div> <div class="links"> <a href="https://arxiv.org/pdf/2403.12459" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/non_neg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/NCL_LIDS_tea.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="luo2023augmentation" class="col-sm-8"> <div class="title">Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning</div> <div class="author"> Rundong Luo<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In ICLR</em>, 2023 </div> <div class="periodical"> We <span style="font-weight: bold;">improved adversarial robustness under AutoAttack by 9%</span> in the unsupervised setting with a dynamic training schedule, without extra computation cost. </div> <div class="links"> <a href="https://openreview.net/pdf?id=0qmwFNJyxCL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/DynACL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <special class="badge rounded w-100">Spotlight</special> </div> <div id="zhang2022mae" class="col-sm-8"> <div class="title">How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders</div> <div class="author"> Qi Zhang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS <span style="font-weight: bold;">Spotlight</span> (Top 5%)</em>, 2022 </div> <div class="periodical"> We theoretically explained how masked autoencoders work and revealed their mathematical connections to joint embedding methods, unifying them as a whole. </div> <div class="links"> <a href="https://arxiv.org/pdf/2210.08344" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/zhangq327/U-MAE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/NeurIPS2022_mae.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="wang2022chaos" class="col-sm-8"> <div class="title">Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap</div> <div class="author"> <u>Yifei Wang</u><sup>*</sup>, Qi Zhang<sup>*</sup>, Yisen Wang, Jiansheng Yang, and Zhouchen Lin </div> <div class="periodical"> <em>In ICLR</em>, 2022 </div> <div class="periodical"> <span style="font-weight: bold;">Cited over 130 times</span>. We derived tight generalization bounds for contrastive learning with a new realistic theoretical framework. It derived unsupervised evaluation metrics with 97% correlation to downstream performance. </div> <div class="links"> <a href="http://arxiv.org/pdf/2203.13457" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/zhangq327/ARC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ICLR2022_overlap.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> <special class="badge rounded w-100">Silver Best Paper <br> at ICML-W’21</special> </div> <div id="wang2022cem" class="col-sm-8"> <div class="title">A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training</div> <div class="author"> <u>Yifei Wang</u>, Yisen Wang, Jiansheng Yang, and Zhouchen Lin </div> <div class="periodical"> <em>In ICLR</em>, 2022 </div> <div class="periodical"> <span style="font-weight: bold; color: var(–global-theme-color);">🏆 Silver Best Paper Award</span> at ICML 2021 AdvML workshop<br> From an energy-based perspective, we formulated contrastive learning as a generative model, and established the connection between adversarial training and maximum likelihood, thus briding generative and discriminative models together. </div> <div class="links"> <a href="http://arxiv.org/pdf/2203.13455" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/ICLR2022_CEM.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECML-PKDD</abbr> <special class="badge rounded w-100">Best ML Paper Award</special> </div> <div id="wang2021reparameterized" class="col-sm-8"> <div class="title">Reparameterized Sampling for Generative Adversarial Networks</div> <div class="author"> <u>Yifei Wang</u>, Yisen Wang, Jiansheng Yang, and Zhouchen Lin </div> <div class="periodical"> <em>In ECML-PKDD</em>, 2021 </div> <div class="periodical"> <span style="font-weight: bold;">🏆 Best ML Paper Award (1/685)</span>, invited to <i>Machine Learning</i><br> We explored using GAN discriminator (as a good reward model) to bootstrap sample quality through an efficient MCMC algorithm, which not only guarantees theoretical convergence but also improves sample efficiency and quality in practice. </div> <div class="links"> <a href="https://arxiv.org/pdf/2107.00352" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/yifeiwang77/repgan" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ECML2021_REPGAN_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>