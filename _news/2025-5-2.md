---
layout: post
date: 2025-5-1
inline: true
related_posts: false
---

3 papers were accepted at ICML 2025. We proposed [CSR](https://www.arxiv.org/pdf/2503.01776) (**Oral Presentation**) that builds state-of-the-art shortening embedding (image/text/multimodal) with sparse coding. We characterized the reasons behind Transformers' [position bias](https://www.arxiv.org/pdf/2502.01951) and enhanced length generalization with [output space alignment](https://openreview.net/pdf?id=sxL3irchez).