---
layout: post
date: 2025-5-1
inline: true
related_posts: false
---

3 papers were accepted at ICML 2025. We inverstigated the theory of Transformers' [postion bias](https://www.arxiv.org/pdf/2502.01951) (lost-in-the-middle, attention sink, RoPE) and improved LLMs' length generalization with [output alignment](https://openreview.net/pdf?id=sxL3irchez). We also proposed [CSR](https://www.arxiv.org/pdf/2503.01776) as a new paradigm to attain adaptive embeddings with sparsity.