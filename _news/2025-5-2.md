---
layout: post
date: 2025-5-1
inline: true
related_posts: false
---

3 papers were accepted at ICML 2025. We proposed [CSR](https://www.arxiv.org/pdf/2503.01776) that builds state-of-the-art shortening embedding models (image/text/multimodal) with sparse coding. We characterized the reasons behind Transformers' [position bias](https://www.arxiv.org/pdf/2502.01951) and how LLMs' length generalization requires [output alignment](https://openreview.net/pdf?id=sxL3irchez). 