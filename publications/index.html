<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publication | Yifei Wang </title> <meta name="author" content="Yifei Wang"> <meta name="description" content="* denotes shared first authorship"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon-192x192.png?df45a378aa715da8c29ebd47ed713002"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yifeiwang77.github.io/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yifei</span> Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publication <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">awards </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">service </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks </a> </li> <li class="nav-item "> <a class="nav-link" href="/misc/">misc </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publication</h1> <p class="post-description">* denotes shared first authorship</p> </header> <article> <p>See full list on <a href="https://scholar.google.com/citations?user=-CLy6YsAAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> <special class="badge rounded w-100">Oral</special> </div> <div id="wen2025beyond" class="col-sm-8"> <div class="title">Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</div> <div class="author"> Tiansheng Wen<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Zequn Zeng, Zhong Peng, Yudi Su, Xinyang Liu, Bo Chen, Hongwei Liu, Stefanie Jegelka, and Chenyu You </div> <div class="periodical"> <em>ICML <span style="font-weight: bold;">Oral Presentation (1%)</span></em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.arxiv.org/pdf/2503.01776" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/neilwen987/CSR_Adaptive_Rep" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="wu2025positionbias" class="col-sm-8"> <div class="title">On the Emergence of Position Bias in Transformers</div> <div class="author"> Xinyi Wu,  <u>Yifei Wang</u>, Stefanie Jegelka, and Ali Jadbabaie </div> <div class="periodical"> <em>ICML</em>, 2025 </div> <div class="periodical"> Featured by <strong><a href="https://news.mit.edu/2025/unpacking-large-language-model-bias-0617" rel="external nofollow noopener" target="_blank">MIT News 📰</a></strong>. </div> <div class="links"> <a href="https://www.arxiv.org/pdf/2502.01951" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=A36u6DB_TgU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="du2025outputalignment" class="col-sm-8"> <div class="title">Long-Short Alignment for Effective Long-Context Modeling in LLMs</div> <div class="author"> Tianqi Du<sup>*</sup>, Haotian Huang<sup>*</sup>,  <u>Yifei Wang</u>, and Yisen Wang </div> <div class="periodical"> <em>ICML</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/pdf?id=sxL3irchez" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/LongShortAlignment" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> </div> <div id="guo2025g1" class="col-sm-8"> <div class="title">G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning</div> <div class="author"> Xiaojun Guo<sup>*</sup>, Ang Li<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Stefanie Jegelka, and Yisen Wang </div> <div class="periodical"> <em>arXiv preprint arXiv:2505.18499</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2505.18499" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/G1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR Workshop</abbr> <special class="badge rounded w-100">Best Paper Runner-up</special> </div> <div id="wu2025more" class="col-sm-8"> <div class="title">When More is Less: Understanding Chain-of-Thought Length in LLMs</div> <div class="author"> Yuyang Wu<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Ziyu Ye, Tianqi Du, Stefanie Jegelka, and Yisen Wang </div> <div class="periodical"> <em>ICLR 2025 Workshop on Reasoning and Planning for LLMs</em>, 2025 </div> <div class="periodical"> <span style="font-weight: bold;">🏆 Best Paper Runner-up Award</span> </div> <div class="links"> <a href="https://arxiv.org/pdf/2502.07266" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> <special2 class="badge rounded w-100">LLM training and eval</special2> </div> <div id="fang2024perplexity" class="col-sm-8"> <div class="title">What is Wrong with Perplexity for Long-context Language Modeling?</div> <div class="author"> Lizhe Fang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang </div> <div class="periodical"> <em>ICLR</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/pdf?id=fL4qWkSmtM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=A36u6DB_TgU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/PKU-ML/LongPPL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> <special2 class="badge rounded w-100">LLM training</special2> </div> <div id="fang2024invariance" class="col-sm-8"> <div class="title">Rethinking Invariance in In-context Learning</div> <div class="author"> Lizhe Fang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Khashayar Gatmiry, Lei Fang, and Yisen Wang </div> <div class="periodical"> <em>ICLR</em>, 2025 </div> <div class="periodical"> We discovered an <strong>expressive invariant in-context learning scheme (InvICL)</strong> that achieves permutation invariance of in-context demonstrations while preserving autoregressive nature and full context awareness at the same time. </div> <div class="links"> <a href="https://openreview.net/pdf?id=q1UyoY3MgJ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="zhuo2025projection" class="col-sm-8"> <div class="title">Can In-context Learning Really Generalize to Out-of-distribution Tasks?</div> <div class="author"> Qixun Wang,  <u>Yifei Wang</u>, Yisen Wang, and Xianghua Ying </div> <div class="periodical"> <em>ICLR</em>, 2025 </div> <div class="periodical"> With controlled experiments, we found that in-context learning still happens only with in-domain tasks and hardly generalizes to novel OOD tasks. In other words, LLMs’ in-context abilities are learned essentially through training data with likewise tasks. </div> <div class="links"> <a href="https://www.arxiv.org/pdf/2410.09695" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="zhuo2025projectioo" class="col-sm-8"> <div class="title">Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness</div> <div class="author"> Qi Zhang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Jingyi Cui, Xiang Pan, Qi Lei, Stefanie Jegelka, and Yisen Wang </div> <div class="periodical"> <em>ICLR</em>, 2025 </div> <div class="periodical"> We found that the merits of <strong>feature monosemanticity</strong> (as studied in mechanistic interpretability) extend beyond interpretability to improving robustness across various challenges like noisy data, limited training examples, and such. </div> <div class="links"> <a href="https://arxiv.org/pdf/2410.21331" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="zhang2025tool" class="col-sm-8"> <div class="title">Projection Head is Secretly an Information Bottleneck</div> <div class="author"> Zhuo Ouyang, Kaiwen Hu, Qi Zhang,  <u>Yifei Wang</u>, and Yisen Wang </div> <div class="periodical"> <em>ICLR</em>, 2025 </div> <div class="periodical"> We showed that projection heads serve as an information bottleneck that prevent features from collapsing toward the pretraining task (e.g. instance classification). </div> <div class="links"> <a href="https://arxiv.org/pdf/2503.00507" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <special class="badge rounded w-100">Best Paper Award<br> at ICML-W’24</special> </div> <div id="wang2024theoretical" class="col-sm-8"> <div class="title">A Theoretical Understanding of Self-Correction through In-context Alignment</div> <div class="author"> <u>Yifei Wang</u><sup>*</sup>, Yuyang Wu<sup>*</sup>, Zeming Wei, Stefanie Jegelka, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> <span style="font-weight: bold;">🏆 Best Paper Award</span> at ICML 2024 ICL Workshop<br> We proposed the first theoretical explanation of how LLM self-correction works (as in OpenAI o1) and showed its effectiveness against social bias and jailbreak attacks. </div> <div class="links"> <a href="https://arxiv.org/pdf/2405.18634" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/yifeiwang77/Self-Correction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="wang2024equivariance" class="col-sm-8"> <div class="title">Understanding the Role of Equivariance in Self-supervised Learning</div> <div class="author"> <u>Yifei Wang</u><sup>*</sup>, Kaiwen Hu<sup>*</sup>, Sharut Gupta, Ziyu Ye, Yisen Wang, and Stefanie Jegelka </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> First theoretical explanation of how equivariant prediction helps self-supervised representation learning – using information theory &amp; probabilistic graphical models. </div> <div class="links"> <a href="https://arxiv.org/pdf/2411.06508" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/kaotty/Understanding-ESSL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <special class="badge rounded w-100">Oral at NeurIPS-W’24</special> <special2 class="badge rounded w-100">Featured by MIT</special2> </div> <div id="wang2024symmetries" class="col-sm-8"> <div class="title">In-Context Symmetries: Self-Supervised Learning through Contextual World Models</div> <div class="author"> Sharut Gupta<sup>*</sup>, Chenyu Wang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Tommi Jaakkola, and Stefanie Jegelka </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> <span style="font-weight: bold;">Oral Presentation (top 4)</span> at NeurIPS 2024 SSL Workshop &amp; <span style="font-weight: bold;">featured by <a href="https://www.csail.mit.edu/news/machines-self-adapt-new-tasks-without-re-training" rel="external nofollow noopener" target="_blank">MIT CSAIL News 📰</a></span><br> We introduced unsupervised test-time adaptation ability to self-supervised learning through a contextual world model designed for joint embedding (JEPA) models. </div> <div class="links"> <a href="https://arxiv.org/pdf/2405.18193" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.csail.mit.edu/news/machines-self-adapt-new-tasks-without-re-training" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/Sharut/In-Context-Symmetries" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="wang2024canonization" class="col-sm-8"> <div class="title">A Canonization Perspective on Invariant and Equivariant Learning</div> <div class="author"> George Ma<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Derek Lim, Stefanie Jegelka, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2405.18378" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/canonicalization" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="wang2024attention" class="col-sm-8"> <div class="title">On the Role of Attention Masks and LayerNorm in Transformers</div> <div class="author"> Xinyi Wu, Amir Ajorlou,  <u>Yifei Wang</u>, Stefanie Jegelka, and Ali Jadbabaie </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2405.18781" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="wang2024dissecting" class="col-sm-8"> <div class="title">Dissecting the Failure of Invariant Learning on Graphs</div> <div class="author"> Qixun Wang,  <u>Yifei Wang</u>, Yisen Wang, and Xianghua Ying </div> <div class="periodical"> <em>In NeurIPS</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/pdf?id=7eFS8aZHAM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/NOVAglow646/NeurIPS24-Invariant-Learning-on-Graphs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS Workshop</abbr> </div> <div id="ye2024reasoning" class="col-sm-8"> <div class="title">Reasoning in Reasoning: A Hierarchical Framework for Better and Faster Neural Theorem Proving</div> <div class="author"> Ziyu Ye, Jiacheng Chen, Jonathan Light,  <u>Yifei Wang</u>, Jiankai Sun, Mac Schwager, Philip Torr, Guohao Li, Yuxin Chen, Kaiyu Yang, Yisong Yue, and Ziniu Hu </div> <div class="periodical"> <em>In NeurIPS 2024 Workshop on Mathematical Reasoning and AI</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/pdf?id=H5hePMXKht" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS Workshop</abbr> </div> <div id="yan2024multifaceted" class="col-sm-8"> <div class="title">The Multi-faceted Monosemanticity in Multimodal Representations</div> <div class="author"> Hanqi Yan, Yulan He, and <u>Yifei Wang</u> (Corresponding Author) </div> <div class="periodical"> <em>In NeurIPS 2024 Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/pdf?id=9NLRpwfLnT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">EMNLP</abbr> </div> <div id="yan2024monosemanticity" class="col-sm-8"> <div class="title">Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective</div> <div class="author"> Hanqi Yan, Yanzheng Xiang, Guangyi Chen,  <u>Yifei Wang</u>, Lin Gui, and Yulan He </div> <div class="periodical"> <em>In EMNLP</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2406.17969v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/hanqi-qi/revisit_monosemanticity" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="zhang2024look" class="col-sm-8"> <div class="title">Look Ahead or Look Around? A Theoretical Comparison Between Autoregressive and Masked Pretraining</div> <div class="author"> Qi Zhang, Tianqi Du, Haotian Huang,  <u>Yifei Wang</u>, and Yisen Wang </div> <div class="periodical"> <em>In ICML</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/pdf?id=2rPoTgEmjV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/LookAheadLookAround" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="li2024oodrobustbench" class="col-sm-8"> <div class="title">OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift</div> <div class="author"> Lin Li,  <u>Yifei Wang</u>, Chawin Sitawarin, and Michael W. Spratling </div> <div class="periodical"> <em>In ICML</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2310.12793" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/OODRobustBench/OODRobustBench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://oodrobustbench.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="zhang2024sharpness" class="col-sm-8"> <div class="title">On the Duality Between Sharpness-Aware Minimization and Adversarial Training</div> <div class="author"> Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen,  <u>Yifei Wang</u>, and Zeming Wei </div> <div class="periodical"> <em>In ICML</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/abs/2402.15152" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/weizeming/SAM_AT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="wang2024nonnegative" class="col-sm-8"> <div class="title">Non-negative Contrastive Learning</div> <div class="author"> <u>Yifei Wang</u><sup>*</sup>, Qi Zhang<sup>*</sup>, Yaoyu Guo, and Yisen Wang </div> <div class="periodical"> <em>In ICLR</em>, 2024 </div> <div class="periodical"> Inspired by NMF, we introduced a simple technique (one-line) that <span style="font-weight: bold;">attains 90% feature sparsity and 10x feature interpretability</span> for self-supervised contrastive learning, with theoretical guarantees on its disentanglement and performance. </div> <div class="links"> <a href="https://arxiv.org/pdf/2403.12459" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/non_neg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/NCL_LIDS_tea.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="wang2024generated" class="col-sm-8"> <div class="title">Do Generated Data Always Help Contrastive Learning?</div> <div class="author"> <u>Yifei Wang</u><sup>*</sup>, Jizhe Zhang<sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In ICLR</em>, 2024 </div> <div class="periodical"> We revealed both theoretically and practically that synthetic data introduces fundamental bias to SSL generalization, but, with an adaptive strategy of data mixing and augmentation, can yield substantial benefits. </div> <div class="links"> <a href="https://arxiv.org/pdf/2403.12448.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/adainf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR Spotlight</abbr> </div> <div id="du2024discrete" class="col-sm-8"> <div class="title">On the Role of Discrete Tokenization in Visual Representation Learning</div> <div class="author"> Tianqi Du<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In ICLR <span style="font-weight: bold;">Spotlight</span></em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/pdf?id=WNLAkjUm19" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/ClusterMIM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <special2 class="badge rounded w-100">Featured by Anthropic</special2> </div> <div id="wei2023jailbreak" class="col-sm-8"> <div class="title">Jailbreak and guard aligned language models with only few in-context demonstrations</div> <div class="author"> Zeming Wei,  <u>Yifei Wang</u>, and Yisen Wang </div> <div class="periodical"> <em>arXiv preprint arXiv:2310.06387</em>, 2023 </div> <div class="periodical"> <span style="font-weight: bold;">Cited over 250 times</span>. Featured and scaled up in <strong><a href="https://www.anthropic.com/research/many-shot-jailbreaking" target="_blank" rel="external nofollow noopener">Anthropic’s blog 📰</a></strong>, where in-context attack successfully jailbroke prominent LLMs including GPT and Claude. </div> <div class="links"> <a href="https://arxiv.org/pdf/2310.06387" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="wang2023balance" class="col-sm-8"> <div class="title">Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective</div> <div class="author"> <u>Yifei Wang</u><sup>*</sup>, Liangchen Li<sup>*</sup>, Jiansheng Yang, Zhouchen Lin, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2310.19360.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/ReBAT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="li2023adversarial" class="col-sm-8"> <div class="title">Adversarial Examples Are Not Real Features</div> <div class="author"> Ang Li<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Yiwen Guo, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2310.18936.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/AdvNotRealFeatures" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="guo2023architecture" class="col-sm-8"> <div class="title">Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning</div> <div class="author"> Xiaojun Guo<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Zeming Wei, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2311.02687.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/ArchitectureMattersGCL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="zhang2023contrastive" class="col-sm-8"> <div class="title">Identifiable Contrastive Learning with Automatic Feature Importance Discovery</div> <div class="author"> Qi Zhang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2310.18904.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/Tri-factor-Contrastive-Learning" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="ma2023laplacian" class="col-sm-8"> <div class="title">Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding</div> <div class="author"> George Ma<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2310.18716.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/LaplacianCanonization" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="zhang2023generalization" class="col-sm-8"> <div class="title">On the Generalization of Multi-modal Contrastive Learning</div> <div class="author"> Qi Zhang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In ICML</em>, 2023 </div> <div class="periodical"> We established the first generalization analysis for multi-modal contrastive learning (e.g., CLIP) and explained how it outperforms self-supervised contrastive learning. </div> <div class="links"> <a href="https://arxiv.org/pdf/2306.04272" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/CLIP-Help-SimCLR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="cui2023rethinking" class="col-sm-8"> <div class="title">Rethinking Weak Supervision in Helping Contrastive Representation Learning</div> <div class="author"> Jingyi Cui<sup>*</sup>, Weiran Huang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In ICML</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2306.04160" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> </div> <div id="wei2023cfa" class="col-sm-8"> <div class="title">CFA: Class-wise Calibrated Fair Adversarial Training</div> <div class="author"> Zeming Wei,  <u>Yifei Wang</u>, Yiwen Guo, and Yisen Wang </div> <div class="periodical"> <em>In CVPR</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2303.14460.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/CFA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TIP</abbr> </div> <div id="chen2023equilibrium" class="col-sm-8"> <div class="title">Equilibrium Image Denoising with Implicit Differentiation</div> <div class="author"> Qi Chen,  <u>Yifei Wang</u>, Zhengyang Geng, Yisen Wang, Jiansheng Yang, and Zhouchen Lin </div> <div class="periodical"> <em>IEEE Transactions on Image Processing (IEEE TIP)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://zhouchenlin.github.io/Publications/2023-TIP-Denoising.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="wang2023message" class="col-sm-8"> <div class="title">A Message Passing Perspective on Learning Dynamics of Contrastive Learning</div> <div class="author"> <u>Yifei Wang</u><sup>*</sup>, Qi Zhang<sup>*</sup>, Tianqi Du, Jiansheng Yang, Zhouchen Lin, and Yisen Wang </div> <div class="periodical"> <em>In ICLR</em>, 2023 </div> <div class="periodical"> We revealed that contrastive learning performs message passing on sample graph, which connects self-supervised learning and graph neural networks as a whole. </div> <div class="links"> <a href="https://openreview.net/pdf?id=VBTJqqWjxMv" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://mp.weixin.qq.com/s/e18pZfee7ffwAHMUBZ_OEg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/PKU-ML/Message-Passing-Contrastive-Learning" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ICLR23_Message_Passing.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="zhuo2023noncontrastive" class="col-sm-8"> <div class="title">Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism</div> <div class="author"> Zhijian Zhuo<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Jinwen Ma, and Yisen Wang </div> <div class="periodical"> <em>In ICLR</em>, 2023 </div> <div class="periodical"> We revealed that various asymmtric designs in non-contrastive learning (BYOL, SimSiam, DINO, SwAV) can be explained from a unified spectral filtering perspective. </div> <div class="links"> <a href="https://openreview.net/pdf?id=cIbjyd2Vcy" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/Rank-Differential-Mechanism" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="luo2023augmentation" class="col-sm-8"> <div class="title">Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning</div> <div class="author"> Rundong Luo<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In ICLR</em>, 2023 </div> <div class="periodical"> We <span style="font-weight: bold;">improved adversarial robustness under AutoAttack by 9%</span> in the unsupervised setting with a dynamic training schedule, without extra computation cost. </div> <div class="links"> <a href="https://openreview.net/pdf?id=0qmwFNJyxCL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/DynACL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="guo2023contranorm" class="col-sm-8"> <div class="title">ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond</div> <div class="author"> Xiaojun Guo<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Tianqi Du<sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In ICLR</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/pdf?id=SM7XkJouWHm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/PKU-ML/ContraNorm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="li2023solver" class="col-sm-8"> <div class="title">Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States</div> <div class="author"> Mingjie Li,  <u>Yifei Wang</u>, Yisen Wang, and Zhouchen Lin </div> <div class="periodical"> <em>In ICLR</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/pdf?id=j3cUWIMsFBN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AAAI</abbr> <special class="badge rounded w-100">Oral</special> </div> <div id="xin2023invariant" class="col-sm-8"> <div class="title">On the Connection between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization</div> <div class="author"> Shiji Xin,  <u>Yifei Wang</u>, Jingtong Su, and Yisen Wang </div> <div class="periodical"> <em>In AAAI</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2212.09082.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <special class="badge rounded w-100">Spotlight</special> </div> <div id="zhang2022mae" class="col-sm-8"> <div class="title">How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders</div> <div class="author"> Qi Zhang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS <span style="font-weight: bold;">Spotlight</span> (Top 5%)</em>, 2022 </div> <div class="periodical"> We theoretically explained how masked autoencoders work and revealed their mathematical connections to joint embedding methods, unifying them as a whole. </div> <div class="links"> <a href="https://arxiv.org/pdf/2210.08344" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/zhangq327/U-MAE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/NeurIPS2022_mae.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS Spotlight</abbr> </div> <div id="wang2022ood" class="col-sm-8"> <div class="title">Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors</div> <div class="author"> Qixun Wang<sup>*</sup>,  <u>Yifei Wang</u><sup>*</sup>, Hong Zhu, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS <span style="font-weight: bold;">Spotlight</span> (Top 5%)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2210.06807" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/NeurIPS2022_OOD.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> <special class="badge rounded w-100">Spotlight</special> </div> <div id="mo2022vision" class="col-sm-8"> <div class="title">When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture</div> <div class="author"> Yichuan Mo, Dongxian Wu,  <u>Yifei Wang</u>, Yiwen Guo, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS <span style="font-weight: bold;">Spotlight</span> (Top 5%)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2210.07540.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS Workshop</abbr> <special class="badge rounded w-100">Oral</special> </div> <div id="cui2022aggnce" class="col-sm-8"> <div class="title">AggNCE: Asymptotically Identifiable Contrastive Learning</div> <div class="author"> Jingyi Cui<sup>*</sup>, Weiran Huang<sup>*</sup>,  <u>Yifei Wang</u>, and Yisen Wang </div> <div class="periodical"> <em>In NeurIPS 2022 Self-supervised Learning Workshop (Oral Representation)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://sslneurips22.github.io/paper_pdfs/paper_68.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="chen2022diffusion" class="col-sm-8"> <div class="title">Optimization-Induced Graph Implicit Nonlinear Diffusion</div> <div class="author"> Qi Chen,  <u>Yifei Wang</u>, Yisen Wang, Jiansheng Yang, and Zhouchen Lin </div> <div class="periodical"> <em>In ICML</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/7qchen/GIND" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="li2022g2cn" class="col-sm-8"> <div class="title">G2CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters</div> <div class="author"> Mingjie Li, Xiaojun Guo,  <u>Yifei Wang</u>, Yisen Wang, and Zhouchen Lin </div> <div class="periodical"> <em>In ICML</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> </div> <div id="wang2022chaos" class="col-sm-8"> <div class="title">Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap</div> <div class="author"> <u>Yifei Wang</u><sup>*</sup>, Qi Zhang<sup>*</sup>, Yisen Wang, Jiansheng Yang, and Zhouchen Lin </div> <div class="periodical"> <em>In ICLR</em>, 2022 </div> <div class="periodical"> <span style="font-weight: bold;">Cited over 130 times</span>. We derived tight generalization bounds for contrastive learning with a new realistic theoretical framework. It derived unsupervised evaluation metrics with 97% correlation to downstream performance. </div> <div class="links"> <a href="http://arxiv.org/pdf/2203.13457" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/zhangq327/ARC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ICLR2022_overlap.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICLR</abbr> <special class="badge rounded w-100">Silver Best Paper <br> at ICML-W’21</special> </div> <div id="wang2022cem" class="col-sm-8"> <div class="title">A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training</div> <div class="author"> <u>Yifei Wang</u>, Yisen Wang, Jiansheng Yang, and Zhouchen Lin </div> <div class="periodical"> <em>In ICLR</em>, 2022 </div> <div class="periodical"> <span style="font-weight: bold; color: var(–global-theme-color);">🏆 Silver Best Paper Award</span> at ICML 2021 AdvML workshop<br> From an energy-based perspective, we formulated contrastive learning as a generative model, and established the connection between adversarial training and maximum likelihood, thus briding generative and discriminative models together. </div> <div class="links"> <a href="http://arxiv.org/pdf/2203.13455" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/ICLR2022_CEM.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="wang2021residual" class="col-sm-8"> <div class="title">Residual Relaxation for Multi-view Representation Learning</div> <div class="author"> <u>Yifei Wang</u>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, and Zhouchen Lin </div> <div class="periodical"> <em>In NeurIPS</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2110.15348" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="/assets/pdf/NeurIPS2021_Prelax_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="wang2021dissecting" class="col-sm-8"> <div class="title">Dissecting the Diffusion Process in Linear Graph Convolutional Networks</div> <div class="author"> <u>Yifei Wang</u>, Yisen Wang, Jiansheng Yang, and Zhouchen Lin </div> <div class="periodical"> <em>In NeurIPS</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://arxiv.org/pdf/2102.10739" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/yifeiwang77/DGC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/NeurIPS2021_DGC_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ECML-PKDD</abbr> <special class="badge rounded w-100">Best ML Paper Award</special> </div> <div id="wang2021reparameterized" class="col-sm-8"> <div class="title">Reparameterized Sampling for Generative Adversarial Networks</div> <div class="author"> <u>Yifei Wang</u>, Yisen Wang, Jiansheng Yang, and Zhouchen Lin </div> <div class="periodical"> <em>In ECML-PKDD</em>, 2021 </div> <div class="periodical"> <span style="font-weight: bold;">🏆 Best ML Paper Award (1/685)</span>, invited to <i>Machine Learning</i><br> We explored using GAN discriminator (as a good reward model) to bootstrap sample quality through an efficient MCMC algorithm, which not only guarantees theoretical convergence but also improves sample efficiency and quality in practice. </div> <div class="links"> <a href="https://arxiv.org/pdf/2107.00352" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/yifeiwang77/repgan" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ECML2021_REPGAN_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>