
@article{fang2024perplexity,
  title={What is Wrong with Perplexity for Long-context Language Modeling?},
  author={Lizhe Fang* and {<u>Yifei Wang</u>}* and Zhaoyang Liu and Chenheng Zhang and Stefanie Jegelka and Jinyang Gao and Bolin Ding and Yisen Wang},
  journal={arXiv preprint arXiv:2410.23771},
  year={2024},
  abbr={arXiv},
  selected = {true},
  note = {We proposed a long-context perplexity measure that emphasizes long-context relevant tokens at training and evaluation, improving benchmark scores on LongBench, LongEval, and RULER by up to 22\%.},
  pdf={https://arxiv.org/pdf/2405.18634},
  code={https://github.com/PKU-ML/LongPPL},
  special = {LLM training & eval},
}

@inproceedings{wang2024theoretical,
  title={A Theoretical Understanding of Self-Correction through In-context Alignment},
  author={{<u>Yifei Wang</u>}* and Yuyang Wu* and Zeming Wei and Stefanie Jegelka and Yisen Wang},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.18634},
  abbr={NeurIPS},
  code={https://github.com/yifeiwang77/Self-Correction},
  selected = {true},
  special = {Best Paper Award<br> at ICML-W'24},
  note = {
  <span style="font-weight: bold;">Best Paper Award</span>  at ICML 2024 ICL Workshop<br>
  We proposed the first theoretical explanation of how LLM self-correction works (as in OpenAI o1) and showed its effectiveness against social bias and jailbreak attacks.}
}

@inproceedings{wang2024equivariance,
  title={Understanding the Role of Equivariance in Self-supervised Learning},
  author={{<u>Yifei Wang</u>}* and Kaiwen Hu* and Sharut Gupta and Ziyu Ye and Yisen Wang and Stefanie Jegelka},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://openreview.net/pdf?id=yLpuruMZHE},
  code={https://github.com/kaotty/Understanding-ESSL},
  note={First theoretical explanation of how equivariant prediction helps self-supervised representation learning -- using information theory & probabilistic graphical models.},
  abbr={NeurIPS},
}

@inproceedings{wang2024symmetries,
  title={In-Context Symmetries: Self-Supervised Learning through Contextual World Models},
  author={Sharut Gupta* and Chenyu Wang* and {<u>Yifei Wang</u>}* and Tommi Jaakkola and Stefanie Jegelka},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.18193},
  code={https://github.com/Sharut/In-Context-Symmetries},
  abbr={NeurIPS},
  special = {Oral at NeurIPS-W'24},
  special2 = {Featured by MIT},
  blog={https://www.csail.mit.edu/news/machines-self-adapt-new-tasks-without-re-training},
  note={<span style="font-weight: bold;">Oral Presentation (top 4)</span> at NeurIPS 2024 SSL Workshop<br>
  We introduced unsupervised test-time adaptation ability to self-supervised learning through a contextual world model designed for joint embedding (JEPA) models.},
  selected = {true},
}


@article{wei2023jailbreak,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and {<u>Yifei Wang</u>} and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.06387},
  abbr={arXiv},
  pdf={https://arxiv.org/pdf/2310.06387},
  special2 = {Featured by Anthropic},
  note = {<span style="font-weight: bold;">Cited over 150 times</span>. Featured and scaled up in <a href="https://www.anthropic.com/research/many-shot-jailbreaking" target="_blank">Anthropic's research blog</a>, where in-context attack successfully jailbroke prominent LLMs including GPT and Claude.},
  selected = {true},
  year={2023}
}

@inproceedings{wang2024canonization,
  title={A Canonization Perspective on Invariant and Equivariant Learning},
  author={George Ma* and {<u>Yifei Wang</u>}* and Derek Lim and Stefanie Jegelka and Yisen Wang},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.18378},
  code={https://github.com/PKU-ML/canonicalization},
  abbr={NeurIPS},
}

@inproceedings{wang2024attention,
  title={On the Role of Attention Masks and LayerNorm in Transformers},
  author={Xinyi Wu and Amir Ajorlou and {<u>Yifei Wang</u>} and Stefanie Jegelka and Ali Jadbabaie},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://arxiv.org/pdf/2405.18781},
  abbr={NeurIPS},
}

@inproceedings{wang2024dissecting,
  title={Dissecting the Failure of Invariant Learning on Graphs},
  author={Qixun Wang and {<u>Yifei Wang</u>} and Yisen Wang and Xianghua Ying},
  booktitle={NeurIPS},
  year={2024},
  pdf={https://openreview.net/pdf?id=7eFS8aZHAM},
  code={https://github.com/NOVAglow646/NeurIPS24-Invariant-Learning-on-Graphs},
  abbr={NeurIPS},
}

@inproceedings{ye2024reasoning,
  author = {Ziyu Ye and Jiacheng Chen and Jonathan Light and {<u>Yifei Wang</u>} and Jiankai Sun and Mac Schwager and Philip Torr and Guohao Li and Yuxin Chen and Kaiyu Yang and Yisong Yue and Ziniu Hu},
  title = {Reasoning in Reasoning: A Hierarchical Framework for Better and Faster Neural Theorem Proving},
  booktitle = { NeurIPS 2024 Workshop on Mathematical Reasoning and AI},
  pdf = {https://openreview.net/pdf?id=H5hePMXKht},
  year = {2024},
  abbr = {NeurIPS Workshop},
}
@inproceedings{yan2024multifaceted,
  author = {Hanqi Yan and Yulan He and {<u>Yifei Wang</u>} (Corresponding Author)},
  title = {The Multi-faceted Monosemanticity in Multimodal Representations},
  booktitle = {NeurIPS 2024 Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models},
  year = {2024},
  pdf = {https://openreview.net/pdf?id=9NLRpwfLnT},
  abbr = {NeurIPS Workshop},
}

@inproceedings{yan2024monosemanticity,
  title={Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective},
  author={Hanqi Yan and Yanzheng Xiang and Guangyi Chen and {<u>Yifei Wang</u>} and Lin Gui and Yulan He},
  booktitle={EMNLP},
  year={2024},
  pdf={https://arxiv.org/pdf/2406.17969v1},
  code={https://github.com/hanqi-qi/revisit_monosemanticity},
  abbr={EMNLP},
}

@inproceedings{zhang2024look,
  title={Look Ahead or Look Around? A Theoretical Comparison Between Autoregressive and Masked Pretraining},
  author={Qi Zhang and Tianqi Du and Haotian Huang and {<u>Yifei Wang</u>} and Yisen Wang},
  booktitle={ICML},
  year={2024},
  abbr={ICML},
  pdf={https://openreview.net/pdf?id=2rPoTgEmjV},
  code={https://github.com/PKU-ML/LookAheadLookAround},
}

@inproceedings{li2024oodrobustbench,
  title={OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift},
  author={Lin Li and {<u>Yifei Wang</u>} and Chawin Sitawarin and Michael W. Spratling},
  booktitle={ICML},
  year={2024},
  pdf={https://arxiv.org/pdf/2310.12793},
  code={https://github.com/OODRobustBench/OODRobustBench},
  website={https://oodrobustbench.github.io/},
  abbr={ICML},
}

@inproceedings{zhang2024sharpness,
  title={On the Duality Between Sharpness-Aware Minimization and Adversarial Training},
  author={Yihao Zhang and Hangzhou He and Jingyu Zhu and Huanran Chen and {<u>Yifei Wang</u>} and Zeming Wei},
  booktitle={ICML},
  year={2024},
  pdf={https://arxiv.org/abs/2402.15152},
  code={https://github.com/weizeming/SAM_AT},
  abbr={ICML},
}

@inproceedings{fang2024rethinking,
  title={Rethinking Invariance in In-context Learning},
  author={Lizhe Fang* and {<u>Yifei Wang</u>}* and Khashayar Gatmiry and Lei Fang and Yisen Wang},
  booktitle={ICML Workshop on Theoretical Foundations of Foundation Models (TF2M)},
  year={2024},
  pdf={https://openreview.net/pdf?id=xSDqxxILWg},
  abbr={ICML Workshop},
}

@inproceedings{wang2024nonnegative,
  title={Non-negative Contrastive Learning},
  author={{<u>Yifei Wang</u>}* and Qi Zhang* and Yaoyu Guo and Yisen Wang},
  booktitle={ICLR},
  year={2024},
  pdf={https://arxiv.org/pdf/2403.12459},
  code={https://github.com/PKU-ML/non_neg},
  slides={NCL_LIDS_tea.pdf},
  abbr={ICLR},
  selected = {true},
  note = {Inspired by NMF, we introduced a one-line technique that <span style="font-weight: bold;">attains 90\% feature sparsity and 10x feature interpretability</span> in contrastive learning models.}
}

@inproceedings{wang2024generated,
  title={Do Generated Data Always Help Contrastive Learning?},
  author={{<u>Yifei Wang</u>}* and Jizhe Zhang* and Yisen Wang},
  booktitle={ICLR},
  year={2024},
  pdf={https://arxiv.org/pdf/2403.12448.pdf},
  code={https://github.com/PKU-ML/adainf},
  featured={https://mp.weixin.qq.com/s/MSSzIl3KnvRzgWVN0ZyW6A},
  abbr={ICLR},
  selected = {false},
  note = {We revealed both theoretically and practically that synthetic data introduces fundamental bias to SSL generalization, but, with an adaptive strategy of data mixing and augmentation, can yield substantial benefits.}
}

@inproceedings{du2024discrete,
  title={On the Role of Discrete Tokenization in Visual Representation Learning},
  author={Tianqi Du* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={ICLR <span style="font-weight: bold;">Spotlight</span>},
  year={2024},
  pdf={https://openreview.net/pdf?id=WNLAkjUm19},
  code={https://github.com/PKU-ML/ClusterMIM},
  abbr={ICLR Spotlight},
}

@inproceedings{wang2023balance,
  title={Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective},
  author={{<u>Yifei Wang</u>}* and Liangchen Li* and Jiansheng Yang and Zhouchen Lin and Yisen Wang},
  booktitle={NeurIPS},
  year={2023},
  pdf={https://arxiv.org/pdf/2310.19360.pdf},
  code={https://github.com/PKU-ML/ReBAT},
  abbr={NeurIPS},
}

@inproceedings{li2023adversarial,
  title={Adversarial Examples Are Not Real Features},
  author={Ang Li* and {<u>Yifei Wang</u>}* and Yiwen Guo and Yisen Wang},
  booktitle={NeurIPS},
  year={2023},
  pdf={https://arxiv.org/pdf/2310.18936.pdf},
  code={https://github.com/PKU-ML/AdvNotRealFeatures},
  abbr={NeurIPS},
}

@inproceedings{guo2023architecture,
  title={Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning},
  author={Xiaojun Guo* and {<u>Yifei Wang</u>}* and Zeming Wei and Yisen Wang},
  booktitle={NeurIPS},
  year={2023},
  pdf={https://arxiv.org/pdf/2311.02687.pdf},
  code={https://github.com/PKU-ML/ArchitectureMattersGCL},
  abbr={NeurIPS},
}

@inproceedings{zhang2023contrastive,
  title={Identifiable Contrastive Learning with Automatic Feature Importance Discovery},
  author={Qi Zhang* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={NeurIPS},
  year={2023},
  pdf={https://arxiv.org/pdf/2310.18904.pdf},
  code={https://github.com/PKU-ML/Tri-factor-Contrastive-Learning},
  abbr={NeurIPS},
}

@inproceedings{ma2023laplacian,
  title={Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding},
  author={George Ma* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={NeurIPS},
  year={2023},
  pdf={https://arxiv.org/pdf/2310.18716.pdf},
  code={https://github.com/PKU-ML/LaplacianCanonization},
  abbr={NeurIPS},
}

@inproceedings{zhang2023generalization,
  title={On the Generalization of Multi-modal Contrastive Learning},
  author={Qi Zhang* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={ICML},
  year={2023},
  pdf={https://arxiv.org/pdf/2306.04272},
  code={https://github.com/PKU-ML/CLIP-Help-SimCLR},
  abbr={ICML},
  selected = {false},
  note={We established the first generalization analysis for multi-modal contrastive learning (e.g., CLIP) and explained how it outperforms self-supervised contrastive learning.}
}

@inproceedings{cui2023rethinking,
  title={Rethinking Weak Supervision in Helping Contrastive Representation Learning},
  author={Jingyi Cui* and Weiran Huang* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={ICML},
  year={2023},
  pdf={https://arxiv.org/pdf/2306.04160},
  abbr={ICML},
}

@inproceedings{wei2023cfa,
  title={CFA: Class-wise Calibrated Fair Adversarial Training},
  author={Zeming Wei and {<u>Yifei Wang</u>} and Yiwen Guo and Yisen Wang},
  booktitle={CVPR},
  year={2023},
  pdf={https://arxiv.org/pdf/2303.14460.pdf},
  code={https://github.com/PKU-ML/CFA},
  abbr={CVPR},
}

@article{chen2023equilibrium,
  title={Equilibrium Image Denoising with Implicit Differentiation},
  author={Qi Chen and {<u>Yifei Wang</u>} and Zhengyang Geng and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  journal={IEEE Transactions on Image Processing (IEEE TIP)},
  year={2023},
  pdf={https://zhouchenlin.github.io/Publications/2023-TIP-Denoising.pdf},
  abbr={TIP},
}

@inproceedings{wang2023message,
  title={A Message Passing Perspective on Learning Dynamics of Contrastive Learning},
  author={{<u>Yifei Wang</u>}* and Qi Zhang* and Tianqi Du and Jiansheng Yang and Zhouchen Lin and Yisen Wang},
  booktitle={ICLR},
  year={2023},
  pdf={https://openreview.net/pdf?id=VBTJqqWjxMv},
  code={https://github.com/PKU-ML/Message-Passing-Contrastive-Learning},
  slides={ICLR23_Message_Passing.pdf},
  blog={https://mp.weixin.qq.com/s/e18pZfee7ffwAHMUBZ_OEg},
  abbr={ICLR},
  selected = {false},
  note = {We revealed that contrastive learning performs message passing on sample graph, which connects self-supervised learning and graph neural networks as a whole.}
}

@inproceedings{zhuo2023noncontrastive,
  title={Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism},
  author={Zhijian Zhuo* and {<u>Yifei Wang</u>}* and Jinwen Ma and Yisen Wang},
  booktitle={ICLR},
  year={2023},
  pdf={https://openreview.net/pdf?id=cIbjyd2Vcy},
  code={https://github.com/PKU-ML/Rank-Differential-Mechanism},
  abbr={ICLR},
  selected = {false},
  note = {We revealed that various asymmtric designs in non-contrastive learning (BYOL, SimSiam, DINO, SwAV) can be explained from a unified spectral filtering perspective.}
}

@inproceedings{luo2023augmentation,
  title={Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning},
  author={Rundong Luo* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={ICLR},
  year={2023},
  pdf={https://openreview.net/pdf?id=0qmwFNJyxCL},
  code={https://github.com/PKU-ML/DynACL},
  abbr={ICLR},
  selected = {true},
  note = {We <span style="font-weight: bold;">improved adversarial robustness under AutoAttack by 9%</span> in the unsupervised setting with a dynamic training schedule, without extra computation cost.}
}

@inproceedings{guo2023contranorm,
  title={ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond},
  author={Xiaojun Guo* and {<u>Yifei Wang</u>}* and Tianqi Du* and Yisen Wang},
  booktitle={ICLR},
  year={2023},
  pdf={https://openreview.net/pdf?id=SM7XkJouWHm},
  code={https://github.com/PKU-ML/ContraNorm},
  abb={ICLR},
}

@inproceedings{li2023solver,
  title={Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States},
  author={Mingjie Li and {<u>Yifei Wang</u>} and Yisen Wang and Zhouchen Lin},
  booktitle={ICLR},
  year={2023},
  pdf={https://openreview.net/pdf?id=j3cUWIMsFBN},
  abbr={ICLR},
}


@inproceedings{xin2023invariant,
  title={On the Connection between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization},
  author={Shiji Xin and {<u>Yifei Wang</u>} and Jingtong Su and Yisen Wang},
  booktitle={AAAI},
  year={2023},
  special={Oral},
  pdf={https://arxiv.org/pdf/2212.09082.pdf},
  abbr={AAAI},
}

@inproceedings{zhang2022mae,
  title={How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders},
  author={Qi Zhang* and {<u>Yifei Wang</u>}* and Yisen Wang},
  booktitle={NeurIPS <span style="font-weight: bold;">Spotlight</span> (Top 5%)},
  year={2022},
  pdf={https://arxiv.org/pdf/2210.08344},
  code={https://github.com/zhangq327/U-MAE},
  slides={NeurIPS2022_mae.pdf},
  abbr={NeurIPS},
  special={Spotlight},
  selected = {true},
  note = {We theoretically explained how masked autoencoders work and revealed their mathematical connections to joint embedding methods, unifying them as a whole. }
}

@inproceedings{wang2022ood,
  title={Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors},
  author={Qixun Wang* and {<u>Yifei Wang</u>}* and Hong Zhu and Yisen Wang},
  booktitle={NeurIPS <span style="font-weight: bold;">Spotlight</span> (Top 5%)},
  year={2022},
  pdf={https://arxiv.org/pdf/2210.06807},
  code={https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD},
  slides={NeurIPS2022_OOD.pdf},
  abbr={NeurIPS Spotlight},
}

@inproceedings{mo2022vision,
  title={When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture},
  author={Yichuan Mo and Dongxian Wu and {<u>Yifei Wang</u>} and Yiwen Guo and Yisen Wang},
  booktitle={NeurIPS <span style="font-weight: bold;">Spotlight</span> (Top 5%)},
  year={2022},
  pdf={https://arxiv.org/pdf/2210.07540.pdf},
  code={https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers},
  abbr={NeurIPS},
  special={Spotlight},
}


@inproceedings{cui2022aggnce,
  title={AggNCE: Asymptotically Identifiable Contrastive Learning},
  author={Jingyi Cui* and Weiran Huang* and {<u>Yifei Wang</u>} and Yisen Wang},
  booktitle={NeurIPS SSL Workshop},
  year={2022},
  special={Oral},
  pdf={https://sslneurips22.github.io/paper_pdfs/paper_68.pdf},
  abbr={NeurIPS Workshop},
}

@inproceedings{chen2022diffusion,
  title={Optimization-Induced Graph Implicit Nonlinear Diffusion},
  author={Qi Chen and {<u>Yifei Wang</u>} and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={ICML},
  year={2022},
  pdf={https://proceedings.mlr.press/v162/chen22z/chen22z.pdf},
  code={https://github.com/7qchen/GIND},
  abbr={ICML},
}

@inproceedings{li2022g2cn,
  title={G2CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters},
  author={Mingjie Li and Xiaojun Guo and {<u>Yifei Wang</u>} and Yisen Wang and Zhouchen Lin},
  booktitle={ICML},
  year={2022},
  pdf={https://proceedings.mlr.press/v162/li22h/li22h.pdf},
  abbr={ICML},
}

@inproceedings{wang2022chaos,
  title={Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap},
  author={{<u>Yifei Wang</u>}* and Qi Zhang* and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={ICLR},
  year={2022},
  pdf={http://arxiv.org/pdf/2203.13457},
  code={https://github.com/zhangq327/ARC},
  slides={ICLR2022_overlap.pdf},
  abbr={ICLR},
  selected = {true},
  note = {<span style="font-weight: bold;">Cited over 130 times</span>. We derived tight generalization bounds for contrastive learning with a new realistic theoretical framework. It derived unsupervised evaluation metrics  with 97% correlation to downstream performance.}
}

@inproceedings{wang2022cem,
  title={A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training},
  author={{<u>Yifei Wang</u>} and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={ICLR},
  year={2022},
  pdf={http://arxiv.org/pdf/2203.13455},
  slides={ICLR2022_CEM.pdf},
  abbr={ICLR},
  selected = {true},
  special = {Silver Best Paper <br> at ICML-W'21},
  note = {
  <span style="font-weight: bold; color: var(--global-theme-color);">Silver Best Paper Award</span> at ICML 2021 AdvML workshop<br>
  From an energy-based perspective, we formulated contrastive learning as a generative model, and  established the connection between adversarial training and maximum likelihood, thus briding generative and discriminative models together.}
}

@inproceedings{wang2021residual,
  title={Residual Relaxation for Multi-view Representation Learning},
  author={{<u>Yifei Wang</u>} and Zhengyang Geng and Feng Jiang and Chuming Li and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={NeurIPS},
  year={2021},
  pdf={https://arxiv.org/pdf/2110.15348},
  slides={NeurIPS2021_Prelax_slides.pdf},
  blog={https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw},
  abbr={NeurIPS},
}

@inproceedings{wang2021dissecting,
  title={Dissecting the Diffusion Process in Linear Graph Convolutional Networks},
  author={{<u>Yifei Wang</u>} and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={NeurIPS},
  year={2021},
  pdf={https://arxiv.org/pdf/2102.10739},
  code={https://github.com/yifeiwang77/DGC},
  slides={NeurIPS2021_DGC_slides.pdf},
  blog={https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA},
  abbr={NeurIPS},
}

@inproceedings{wang2021reparameterized,
  title={Reparameterized Sampling for Generative Adversarial Networks},
  author={{<u>Yifei Wang</u>} and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle={ECML-PKDD},
  year={2021},
  pdf={https://arxiv.org/pdf/2107.00352},
  code={https://github.com/yifeiwang77/repgan},
  slides={ECML2021_REPGAN_slides.pdf},
  media={https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ},
  talk={https://www.bilibili.com/video/BV1sL4y167gj},
  abbr={ECML-PKDD},
  special={Best ML Paper Award},
  selected = {true},
  note = {
  <span style="font-weight: bold;">Best ML Paper Award (1/685)</span>, invited to <i>Machine Learning</i><br>
  We explored using GAN discriminator (as a good reward model) to bootstrap sample quality through an efficient MCMC algorithm, which not only guarantees theoretical  convergence but also improves sample efficiency and quality in practice.}
}
