---
layout: about
title: about
permalink: /
subtitle: Postdoctoral researcher at <a href='https://www.csail.mit.edu/'>MIT CSAIL</a>, advised by <a href="https://people.csail.mit.edu/stefje/">Stefanie Jegelka</a>.

profile:
  align: right
  image: avatar.JPG
  image_circular: false # crops the image to make it circular
social: true # includes social icons at the bottom of the page
news: true # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
---

I am interested in understanding and building AI models with good representation of the world, with a focus on unsupervised learning and language models these days (see [research](/research)). I obtained my PhD in Applied Mathematics from Peking University in 2023, advised by [Yisen Wang](https://yisenwang.github.io), [Zhouchen Lin](https://zhouchenlin.github.io/), [Jiansheng Yang](https://www.math.pku.edu.cn/jsdw/js_20180628175159671361/y_20180628175159671361/69984.htm). I did my undergraduate at School of Mathematical Sciences at Peking University as well. My first-author papers received [4 best paper awards](./awards) and I served as an Area Chair for ICLR 2024 and 2025.

I like solving mysterious machine learning puzzles with major impacts, such as, [why overthinking harms LLM reasoning](https://arxiv.org/pdf/2502.07266), [why Transformers have position bias](https://www.arxiv.org/pdf/2502.01951), [why DINO features won't collapse](https://arxiv.org/pdf/2303.02387), [why MAE learns good features](https://arxiv.org/pdf/2210.08344), [why adversarial training severely overfits](https://arxiv.org/pdf/2310.19360.pdf), and [why robust models become generative](https://arxiv.org/pdf/2203.13455). Don't wanna bother reading papers? Buy me a coffee and Iâ€™ll give you a 5-min walkthrough!

<!-- I occasionally post latest research highlights on [Twitter / X](https://x.com/yifeiwang77/highlights).  -->

<!-- 
My goal is to develop general-purpose models with minimal human efforts, which drives my persistent interests into self-supervised learning of foundation models. My research has contributed to unveiling the key principles underlying these foundation models and designing efficient algorithms to improve their capabilities and safety:
<!-- I work on theoretical principles of foundation models (generative models & representation models) and practical algorithms for improving their capabilities and safety: 
- **Mathematical Principles of Foundation Models**. We established theoretical foundations for a broad spectrum of *Self-Supervised Learning (SSL)* methods that are at the heart of foundation models, from contrastive [[1](http://arxiv.org/pdf/2203.13457), [2](https://openreview.net/pdf?id=VBTJqqWjxMv)], non-contrastive [[3](https://openreview.net/pdf?id=cIbjyd2Vcy)], autoregressive [[4](https://openreview.net/pdf?id=2rPoTgEmjV)], reconstructive [[5](https://arxiv.org/pdf/2210.08344)], to predictive [[6](https://openreview.net/pdf?id=yLpuruMZHE)] approaches. Our recent work further pioneered the first rigorous theory [[7](https://arxiv.org/pdf/2405.18634)] for the *self-correction ability* of LLMs, a key mechanism for LLM reasoning at test time.
- **Improving Model Capabilities**. We leveraged these principles to "debug" and "boost" foundation models. We generalized self-supervised learning to self-adapt to new tasks without retraining [[8](https://arxiv.org/pdf/2405.18193)] ([featured by MIT](https://www.csail.mit.edu/news/machines-self-adapt-new-tasks-without-re-training)), proposed adaptive use of AI-generated data to circumvent data shortage [[9](https://arxiv.org/pdf/2403.12448.pdf)], and significantly enhanced LLMs' long-context understanding through self-identifying key tokens [[10](https://arxiv.org/pdf/2410.23771)].
- **Safe and Trustworthy AI**. We developed principled understandings and algorithms for adversarial robustness [[11](http://arxiv.org/pdf/2203.13455), [12](https://arxiv.org/pdf/2210.07540.pdf), [13](https://arxiv.org/pdf/2310.19360.pdf), [14](https://arxiv.org/pdf/2310.18936.pdf)], interpretability [[15](https://arxiv.org/pdf/2310.18904.pdf), [16](https://arxiv.org/pdf/2403.12459)], and domain generalization [[17](https://arxiv.org/pdf/2210.06807), [18](https://arxiv.org/pdf/2212.09082.pdf), [19](https://arxiv.org/pdf/2310.12793)]. In DynACL [[20](https://arxiv.org/abs/2303.01289)], we built the first self-supervised model that is as robust as the supervised one. We firstly showed that LLMs' core emergent abilities, in-context learning [[21](https://arxiv.org/pdf/2310.06387)] and self-correction [[7](https://arxiv.org/pdf/2405.18634)], can play important roles in safety tasks like jailbreaking, which was [featured and scaled up by Anthropic](https://www.anthropic.com/research/many-shot-jailbreaking).
-->



