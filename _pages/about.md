---
layout: about
title: about
permalink: /

profile:
  align: right
  image: avatar.JPG
  image_circular: false # crops the image to make it circular
social: true # includes social icons at the bottom of the page
news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
---


I am currently a Member of Technical Staff at the <a href="https://www.amazon.science/blog/amazon-opens-new-ai-lab-in-san-francisco-focused-on-long-term-research-bets">Amazon AGI SF Lab</a>, led by <a href="https://www.davidluan.com/">David Luan</a> and <a href="https://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>.  I work with an amazing team pushing long-term frontier research in large language models, reinforcement learning, and autonomous agents.
<!-- My research focuses on frontier problems in large language models, reinforcement learning, and autonomous agents. -->

Previously, I was a postdoctoral researcher at <a href="https://www.csail.mit.edu/">MIT CSAIL</a>, advised by <a href="https://people.csail.mit.edu/stefje/">Stefanie Jegelka</a>.  I received my Ph.D. in Applied Mathematics from Peking University, advised by  [Yisen Wang](https://yisenwang.github.io), [Zhouchen Lin](https://zhouchenlin.github.io/), and  [Jiansheng Yang](https://www.math.pku.edu.cn/jsdw/js_20180628175159671361/y_20180628175159671361/69984.htm).  I also completed my B.S. and B.A. at Peking University.

My research interests lie broadly in unsupervised learning, representation learning, and reinforcement learning, with the overarching goal of developing more scalable and generalizable learning paradigms. My work has received [4 best paper awards](./awards) and has been featured by  [MIT News](https://news.mit.edu/2025/unpacking-large-language-model-bias-0617) and  [Anthropic](https://www.anthropic.com/research/many-shot-jailbreaking). I serve as an Area Chair for ICLR and ICML.


<!-- with application to building generic and reliable AI agents. -->
<!-- I am interested in unsupervised learning, representation learning, and reasoning.   methods for scaling foundation models  -->
<!-- with interests in unsupervised learning  -->
<!-- representation learning, reasoning, a understanding and developing scalable machine learning methods. My past research includes self-supervised learning, reasoning, -->
