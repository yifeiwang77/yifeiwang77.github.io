---
layout: about
title: about
permalink: /
subtitle: Postdoctoral researcher at <a href='https://www.csail.mit.edu/'>MIT CSAIL</a>, advised by <a href="https://people.csail.mit.edu/stefje/">Stefanie Jegelka</a>.

profile:
  align: right
  image: avatar.JPG
  image_circular: false # crops the image to make it circular
social: true # includes social icons at the bottom of the page
news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
---
My goal is to develop models that learn from massive data with minimal human efforts, which drives my persistent interests in self-supervised foundation models. My research has contributed to unveiling the key principles underlying these foundation models and designing efficient algorithms to improve their capabilities and safety:
<!-- I work on theoretical principles of foundation models (generative models & representation models) and practical algorithms for improving their capabilities and safety: -->
- **Mathematical Principles of Foundation Models**. We established theoretical foundations for a broad spectrum of *Self-Supervised Learning (SSL)* methods that are at the heart of foundation models, from contrastive [[1](http://arxiv.org/pdf/2203.13457), [2](https://openreview.net/pdf?id=VBTJqqWjxMv)], non-contrastive [[3](https://openreview.net/pdf?id=cIbjyd2Vcy)], autoregressive [[4](https://openreview.net/pdf?id=2rPoTgEmjV)], reconstructive [[5](https://arxiv.org/pdf/2210.08344)], to predictive [[6](https://openreview.net/pdf?id=yLpuruMZHE)] approaches. Our recent work further pioneered the first rigorous theory [[7](https://arxiv.org/pdf/2405.18634)] for the *test-time self-correction ability* of LLMs, a key mechanism for LLM reasoning.
- **Improving Model Capabilities**. We leveraged these principles to "debug" and "boost" foundation models. We generalized self-supervised learning to be able to self-adapt to new tasks without retraining [[8](https://arxiv.org/pdf/2405.18193)] ([featured by MIT](https://www.csail.mit.edu/news/machines-self-adapt-new-tasks-without-re-training)), proposed adaptive use of AI-generated data to circumvent data shortage [[9](https://arxiv.org/pdf/2403.12448.pdf)], and significantly enhanced LLMs' long-context understanding through self-identifying key tokens [[10](https://arxiv.org/pdf/2410.23771)].
- **Safe and Trustworthy AI**. We developed principled understandings and algorithms for adversarial robustness [[11](http://arxiv.org/pdf/2203.13455), [12](https://arxiv.org/pdf/2210.07540.pdf), [13](https://arxiv.org/pdf/2310.19360.pdf), [14](https://arxiv.org/pdf/2310.18936.pdf)], interpretability [[15](https://arxiv.org/pdf/2310.18904.pdf), [16](https://arxiv.org/pdf/2403.12459)], and domain generalization [[17](https://arxiv.org/pdf/2210.06807), [18](https://arxiv.org/pdf/2212.09082.pdf), [19](https://arxiv.org/pdf/2310.12793)]. In DynACL [[20](https://arxiv.org/abs/2303.01289)], we built the first self-supervised model that is as robust as the supervised one. We firstly showed that LLMs' core emergent abilities, in-context learning [[21](https://arxiv.org/pdf/2310.06387)] and self-correction [[7](https://arxiv.org/pdf/2405.18634)], can play important roles in safety tasks like jailbreaking, which was [featured and scaled up by Anthropic](https://www.anthropic.com/research/many-shot-jailbreaking).
 
My first-author papers received the Best ML Paper Award (1/685) at ECML-PKDD 2021, the Silver Best Paper Award at ICML 2021 workshop, and the Best Paper Award at ICML 2024 workshop. My thesis won CAAI Outstanding Ph.D. Dissertation Runner-Up Award. I have published 43 peer-reviewed papers (38 in NeurIPS, ICLR, and ICML), and I am a (co-)first author on 28 of them.

I served as an organizer for [NeurIPS 2024 Workshop on Red Teaming GenAI](https://redteaming-gen-ai.github.io/) and the [ML Tea Seminar at MIT](https://projects.csail.mit.edu/ml-tea/). I served as an Area Chair for ICLR 2024 and 2025, and as a reviewer for main AI conferences (NeurIPS, ICML, ECML, AISTATS, LoG, CVPR, ACL).

I obtained my PhD in Applied Mathematics from Peking University in 2023, advised by [Yisen Wang](https://yisenwang.github.io), [Zhouchen Lin](https://zhouchenlin.github.io/), [Jiansheng Yang](https://www.math.pku.edu.cn/jsdw/js_20180628175159671361/y_20180628175159671361/69984.htm). Prior to that, I did my undergraduate at School of Mathematical Sciences, Peking University.

**I am on the job market 2024-2025 and actively looking for jobs! Links: [CV](assets/pdf/CV-Yifei-Wang-MIT.pdf) \|  [Research Statement](assets/pdf/Research Statement Yifei Wang.pdf)**


You may find some of my recent research highlights on [X/Twitter](https://x.com/yifeiwang77/highlights).